# --- Required Libraries ---
import os
import json
import gzip
import glob  # For finding files matching a pattern
import argparse
import logging
from collections.abc import Iterable # To check if an object is iterable (safer than checking list directly)

# --- Default Configuration ---
# Assumes the script runs in the same directory as the original script
# and the output data is in a subdirectory named 'github_data_compressed'
DEFAULT_INPUT_DIR = "github_data_compressed"
DEFAULT_OUTPUT_FILE = "extracted_github_links.json"
DEFAULT_LOG_FILE = "link_extractor.log"

# --- Argument Parser Setup ---
parser = argparse.ArgumentParser(description="Extract GitHub URLs from compressed JSONL files.")
parser.add_argument('--input_dir', default=DEFAULT_INPUT_DIR,
                    help='Directory containing the .jsonl.gz files generated by the scraper.')
parser.add_argument('--output_file', default=DEFAULT_OUTPUT_FILE,
                    help='Output JSON file to save the extracted GitHub links.')
parser.add_argument('--log_file', default=DEFAULT_LOG_FILE,
                    help='Path to the log file for this extraction script.')
parser.add_argument('--no_deduplicate', action='store_true',
                    help='If set, do not remove duplicate URLs from the final list.')

args = parser.parse_args()

# Update config based on args
INPUT_DIR = args.input_dir
OUTPUT_FILE = args.output_file
LOG_FILE = args.log_file
DEDUPLICATE = not args.no_deduplicate

# --- Logging Setup ---
# Ensure logs directory exists if a path is specified
log_dir = os.path.dirname(LOG_FILE)
if log_dir and not os.path.exists(log_dir):
    os.makedirs(log_dir)

# Clear previous handlers if any
for handler in logging.root.handlers[:]:
    logging.root.removeHandler(handler)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, mode='w'), # Overwrite log file each run
        logging.StreamHandler()
    ]
)

# --- Main Extraction Logic ---
def extract_links():
    start_time = time.time() # Use time from original script context if needed
    logging.info(f"Starting GitHub link extraction process.")
    logging.info(f"Input directory: {os.path.abspath(INPUT_DIR)}")
    logging.info(f"Output file: {os.path.abspath(OUTPUT_FILE)}")
    logging.info(f"Log file: {os.path.abspath(LOG_FILE)}")
    logging.info(f"Deduplicate links: {DEDUPLICATE}")

    if not os.path.isdir(INPUT_DIR):
        logging.error(f"Input directory not found: {INPUT_DIR}")
        return

    # Find all relevant files using glob
    search_pattern = os.path.join(INPUT_DIR, "repos_*.jsonl.gz")
    input_files = glob.glob(search_pattern)

    if not input_files:
        logging.warning(f"No '.jsonl.gz' files matching the pattern found in directory: {INPUT_DIR}")
        # Create an empty output file if no input files are found
        try:
            with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
                json.dump([], f)
            logging.info(f"Created empty output file: {OUTPUT_FILE}")
        except IOError as e:
            logging.error(f"Error creating empty output file {OUTPUT_FILE}: {e}")
        return

    logging.info(f"Found {len(input_files)} potential input files.")

    all_links = []
    processed_files_count = 0
    total_lines_read = 0
    total_links_extracted = 0
    files_with_errors = 0

    for filepath in input_files:
        logging.info(f"--- Processing file: {os.path.basename(filepath)} ---")
        lines_in_file = 0
        links_in_file = 0
        file_had_errors = False
        try:
            # Open the compressed file in text read mode ('rt')
            with gzip.open(filepath, 'rt', encoding='utf-8') as infile:
                for i, line in enumerate(infile):
                    lines_in_file += 1
                    total_lines_read += 1
                    line = line.strip() # Remove leading/trailing whitespace

                    if not line: # Skip empty lines
                        continue

                    try:
                        # Parse the line as JSON
                        record = json.loads(line)

                        # Extract the 'github_url' - safely using .get()
                        link = record.get("github_url")

                        if isinstance(link, str) and link: # Check if it's a non-empty string
                            all_links.append(link)
                            links_in_file += 1
                            total_links_extracted += 1
                        # else: # Optional: Log if URL is missing or not a string
                        #    logging.debug(f"No valid 'github_url' string found in record on line {i+1} of {os.path.basename(filepath)}")

                    except json.JSONDecodeError:
                        logging.warning(f"Skipping invalid JSON on line {i+1} in file {os.path.basename(filepath)}: {line[:100]}...")
                        file_had_errors = True
                    except Exception as e: # Catch other errors during record processing
                         logging.error(f"Error processing record on line {i+1} in file {os.path.basename(filepath)}: {e}")
                         file_had_errors = True

            logging.info(f"Finished processing {os.path.basename(filepath)}. Read {lines_in_file} lines, extracted {links_in_file} links.")
            processed_files_count += 1
            if file_had_errors:
                files_with_errors +=1

        except gzip.BadGzipFile:
             logging.error(f"Error: File {os.path.basename(filepath)} is not a valid gzip file. Skipping.")
             files_with_errors +=1
        except IOError as e:
            logging.error(f"I/O error reading file {os.path.basename(filepath)}: {e}")
            files_with_errors +=1
        except Exception as e: # Catch broader errors opening/reading the file
            logging.error(f"An unexpected error occurred while processing file {os.path.basename(filepath)}: {e}")
            files_with_errors +=1

    logging.info(f"--- Finished processing all files ---")
    logging.info(f"Successfully processed {processed_files_count} out of {len(input_files)} files found.")
    if files_with_errors > 0:
        logging.warning(f"{files_with_errors} file(s) encountered errors during processing.")
    logging.info(f"Total lines read across all files: {total_lines_read}")
    logging.info(f"Total GitHub links extracted (before deduplication): {total_links_extracted}")

    # Deduplicate if requested
    if DEDUPLICATE:
        original_count = len(all_links)
        # Using dict.fromkeys preserves order in Python 3.7+ while being efficient
        unique_links = list(dict.fromkeys(all_links))
        final_links = unique_links
        duplicates_removed = original_count - len(final_links)
        if duplicates_removed > 0:
            logging.info(f"Removed {duplicates_removed} duplicate links.")
        else:
             logging.info("No duplicate links found.")
    else:
        final_links = all_links
        logging.info("Skipping deduplication as requested.")

    logging.info(f"Total unique links to save: {len(final_links)}")

    # Save the extracted links to the output JSON file
    try:
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as outfile:
            # Use indent for better readability of the output JSON
            json.dump(final_links, outfile, indent=2, ensure_ascii=False)
        logging.info(f"Successfully wrote {len(final_links)} links to {OUTPUT_FILE}")
    except IOError as e:
        logging.error(f"Error writing output file {OUTPUT_FILE}: {e}")
    except Exception as e:
        logging.error(f"An unexpected error occurred while writing the output file: {e}")

    end_time = time.time() # Use time from original script context if needed
    total_duration = end_time - start_time
    logging.info(f"Link extraction process finished in {total_duration:.2f} seconds.")

# Need to import time if using it for duration calculation
import time

if __name__ == "__main__":
    extract_links()