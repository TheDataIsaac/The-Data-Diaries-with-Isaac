{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66eea550-8854-441b-b872-c4fe18f3d435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for archive files named '*.jsonl' in 'github_data_compressed'...\n",
      "  - No '*.jsonl' files found. Searching for '*.zip' instead...\n",
      "Found 0 potential archive files. Processing...\n",
      "Error: No suitable archive files (.jsonl or .zip) found in 'github_data_compressed'. Exiting.\n",
      "\n",
      "Error: Could not read any valid data from the archive files in 'github_data_compressed'. Exiting.\n",
      "\n",
      "Concatenating data from all archives...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 86\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Concatenate all the individual DataFrames into one\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConcatenating data from all archives...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 86\u001b[0m api_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_dataframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully combined data from archives. Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_df\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# --- Step 2: Read the scraped data JSONL file ---\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[1;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[1;34m(self, objs, keys)\u001b[0m\n\u001b[0;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json # Used for potential error diagnostics if needed\n",
    "import zipfile # Import the zipfile module\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the path to the folder containing the archive files\n",
    "# (These might be named .jsonl but are actually zip archives)\n",
    "api_data_folder = Path(\"github_data_compressed\")\n",
    "\n",
    "# Define the path to the single scraped data JSONL file\n",
    "scraped_data_file = Path(\"scraped_data.jsonl\")\n",
    "\n",
    "# Define the name for the final combined output file\n",
    "output_file = Path(\"combined_github_data.jsonl\")\n",
    "\n",
    "# --- Step 1: Read and combine all JSONL files from the archives ---\n",
    "\n",
    "# Look for files named *.jsonl first, as shown in the first image\n",
    "print(f\"Searching for archive files named '*.jsonl' in '{api_data_folder}'...\")\n",
    "all_api_archive_files = list(api_data_folder.glob('*.jsonl'))\n",
    "\n",
    "# If no '*.jsonl' files are found, try looking for '*.zip' as a fallback\n",
    "if not all_api_archive_files:\n",
    "    print(f\"  - No '*.jsonl' files found. Searching for '*.zip' instead...\")\n",
    "    all_api_archive_files = list(api_data_folder.glob('*.zip'))\n",
    "\n",
    "api_dataframes = [] # List to hold DataFrames from each archive\n",
    "\n",
    "print(f\"Found {len(all_api_archive_files)} potential archive files. Processing...\")\n",
    "\n",
    "if not all_api_archive_files:\n",
    "    print(f\"Error: No suitable archive files (.jsonl or .zip) found in '{api_data_folder}'. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "for archive_file_path in all_api_archive_files: # Loop through found archive files\n",
    "    print(f\"  - Processing '{archive_file_path.name}'...\")\n",
    "    try:\n",
    "        # Open the archive file (treats it as a zip regardless of name)\n",
    "        with zipfile.ZipFile(archive_file_path, 'r') as zip_ref:\n",
    "            # Find the .jsonl file(s) inside the zip archive\n",
    "            jsonl_files_in_zip = [f for f in zip_ref.namelist() if f.endswith('.jsonl')]\n",
    "\n",
    "            if not jsonl_files_in_zip:\n",
    "                print(f\"    - Warning: No .jsonl file found inside '{archive_file_path.name}'. Skipping.\")\n",
    "                continue # Skip to the next archive file\n",
    "\n",
    "            # Determine which jsonl file to use if multiple are found\n",
    "            if len(jsonl_files_in_zip) > 1:\n",
    "                # Attempt to find a jsonl file matching the archive name (minus extension)\n",
    "                base_name = archive_file_path.stem # Gets filename without final extension\n",
    "                matching_files = [f for f in jsonl_files_in_zip if Path(f).stem == base_name]\n",
    "                if matching_files:\n",
    "                    jsonl_filename_inside_zip = matching_files[0]\n",
    "                    print(f\"    - Warning: Multiple .jsonl files found. Using matching name: '{jsonl_filename_inside_zip}'.\")\n",
    "                else:\n",
    "                    jsonl_filename_inside_zip = jsonl_files_in_zip[0] # Default to first if no match\n",
    "                    print(f\"    - Warning: Multiple .jsonl files found. No clear match. Using the first one: '{jsonl_filename_inside_zip}'.\")\n",
    "            else:\n",
    "                 jsonl_filename_inside_zip = jsonl_files_in_zip[0] # Only one file found\n",
    "\n",
    "            # Open the specific .jsonl file from within the zip archive\n",
    "            with zip_ref.open(jsonl_filename_inside_zip) as jsonl_file:\n",
    "                # Read the JSONL file directly from the zip archive into a pandas DataFrame\n",
    "                df = pd.read_json(jsonl_file, lines=True)\n",
    "                if not df.empty:\n",
    "                    api_dataframes.append(df)\n",
    "                    print(f\"    - Successfully read '{jsonl_filename_inside_zip}' ({len(df)} records)\")\n",
    "                else:\n",
    "                    print(f\"    - Warning: '{jsonl_filename_inside_zip}' inside '{archive_file_path.name}' is empty or could not be parsed.\")\n",
    "\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"  - Error: '{archive_file_path.name}' is not a valid zip file or is corrupted. Skipping.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  - Error processing archive file {archive_file_path}: {e}\")\n",
    "        # Optional: Add more robust error handling here if needed\n",
    "\n",
    "# Check if any data was successfully read\n",
    "if not api_dataframes:\n",
    "    print(f\"\\nError: Could not read any valid data from the archive files in '{api_data_folder}'. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Concatenate all the individual DataFrames into one\n",
    "print(\"\\nConcatenating data from all archives...\")\n",
    "api_df = pd.concat(api_dataframes, ignore_index=True)\n",
    "print(f\"Successfully combined data from archives. Shape: {api_df.shape}\")\n",
    "\n",
    "# --- Step 2: Read the scraped data JSONL file ---\n",
    "print(f\"\\nReading scraped data file '{scraped_data_file}'...\")\n",
    "try:\n",
    "    scraped_df = pd.read_json(scraped_data_file, lines=True)\n",
    "    print(f\"Successfully read scraped data. Shape: {scraped_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Scraped data file '{scraped_data_file}' not found. Exiting.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error reading scraped data file {scraped_data_file}: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Step 3: Prepare for Merging ---\n",
    "api_key_col = 'github_url'\n",
    "scraped_key_col = 'url'\n",
    "\n",
    "# Check if the key columns exist before attempting merge\n",
    "if api_key_col not in api_df.columns:\n",
    "    print(f\"Error: Key column '{api_key_col}' not found in the combined API data. Available columns: {api_df.columns.tolist()}\")\n",
    "    exit()\n",
    "if scraped_key_col not in scraped_df.columns:\n",
    "     print(f\"Error: Key column '{scraped_key_col}' not found in the scraped data. Available columns: {scraped_df.columns.tolist()}\")\n",
    "     exit()\n",
    "\n",
    "# Rename the key column in the scraped data to match the API data for merging\n",
    "print(f\"\\nRenaming '{scraped_key_col}' to '{api_key_col}' in scraped data for merging.\")\n",
    "scraped_df.rename(columns={scraped_key_col: api_key_col}, inplace=True)\n",
    "\n",
    "# --- Step 4: Merge the DataFrames ---\n",
    "print(f\"Merging the two datasets using '{api_key_col}' as the key...\")\n",
    "\n",
    "# Perform a 'left' merge: Keep all rows from api_df (the combined API data)\n",
    "# and add matching columns from scraped_df based on the 'github_url'.\n",
    "# If a URL from api_df doesn't have a match in scraped_df, the new columns\n",
    "# for that row will be filled with NaN (Not a Number).\n",
    "merged_df = pd.merge(api_df, scraped_df, on=api_key_col, how='left')\n",
    "\n",
    "print(f\"Merge complete. Shape of final DataFrame: {merged_df.shape}\")\n",
    "\n",
    "# --- Step 5: Save the Combined Data ---\n",
    "print(f\"\\nSaving the merged data to '{output_file}'...\")\n",
    "try:\n",
    "    # Save the merged DataFrame to a new JSONL file\n",
    "    # orient='records' makes each row a JSON object\n",
    "    # lines=True ensures it's saved in the JSON Lines format\n",
    "    # force_ascii=False helps prevent unicode characters from being escaped (optional)\n",
    "    merged_df.to_json(output_file, orient='records', lines=True, force_ascii=False)\n",
    "    print(f\"Successfully saved combined data to '{output_file}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving the output file: {e}\")\n",
    "\n",
    "print(\"\\nScript finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4e71469-6eea-4b5f-a6fe-6af13acddb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for folders inside 'github_data_compressed'...\n",
      "Found 0 folders. Processing...\n",
      "Error: No folders found in 'github_data_compressed'. Exiting.\n",
      "\n",
      "Error: No valid data found in any folders inside 'github_data_compressed'. Exiting.\n",
      "\n",
      "Concatenating data from all folders...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Concatenate all the individual DataFrames into one\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConcatenating data from all folders...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m api_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_dataframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully combined data. Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_df\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# --- Step 2: Read the scraped data JSONL file ---\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[1;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[1;34m(self, objs, keys)\u001b[0m\n\u001b[0;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Configuration ---\n",
    "api_data_folder = Path(\"github_data_compressed\")  # Folder with subfolders containing .jsonl files\n",
    "scraped_data_file = Path(\"scraped_data.jsonl\")    # File with scraped data\n",
    "output_file = Path(\"combined_github_data.jsonl\")  # Final output file\n",
    "\n",
    "# --- Step 1: Read and combine all JSONL files from the folders ---\n",
    "print(f\"Searching for folders inside '{api_data_folder}'...\")\n",
    "\n",
    "# Each subfolder is expected to contain a .jsonl file with the same name as the folder\n",
    "all_api_folders = [f for f in api_data_folder.iterdir() if f.is_dir()]\n",
    "api_dataframes = []\n",
    "\n",
    "print(f\"Found {len(all_api_folders)} folders. Processing...\")\n",
    "\n",
    "if not all_api_folders:\n",
    "    print(f\"Error: No folders found in '{api_data_folder}'. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "for folder_path in all_api_folders:\n",
    "    print(f\"  - Processing folder: '{folder_path.name}'\")\n",
    "\n",
    "    # Construct the expected .jsonl file path inside this folder\n",
    "    inner_jsonl_file = folder_path / f\"{folder_path.name}\"\n",
    "\n",
    "    if inner_jsonl_file.exists():\n",
    "        try:\n",
    "            df = pd.read_json(inner_jsonl_file, lines=True)\n",
    "            if not df.empty:\n",
    "                api_dataframes.append(df)\n",
    "                print(f\"    - Successfully read {len(df)} records from '{inner_jsonl_file.name}'\")\n",
    "            else:\n",
    "                print(f\"    - Warning: '{inner_jsonl_file.name}' is empty.\")\n",
    "        except Exception as e:\n",
    "            print(f\"    - Error reading '{inner_jsonl_file.name}': {e}\")\n",
    "    else:\n",
    "        print(f\"    - Warning: File '{inner_jsonl_file.name}' not found in '{folder_path.name}'. Skipping.\")\n",
    "\n",
    "if not api_dataframes:\n",
    "    print(f\"\\nError: No valid data found in any folders inside '{api_data_folder}'. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Concatenate all the individual DataFrames into one\n",
    "print(\"\\nConcatenating data from all folders...\")\n",
    "api_df = pd.concat(api_dataframes, ignore_index=True)\n",
    "print(f\"Successfully combined data. Shape: {api_df.shape}\")\n",
    "\n",
    "# --- Step 2: Read the scraped data JSONL file ---\n",
    "print(f\"\\nReading scraped data file '{scraped_data_file}'...\")\n",
    "try:\n",
    "    scraped_df = pd.read_json(scraped_data_file, lines=True)\n",
    "    print(f\"Successfully read scraped data. Shape: {scraped_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Scraped data file '{scraped_data_file}' not found. Exiting.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error reading scraped data file {scraped_data_file}: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Step 3: Prepare for Merging ---\n",
    "api_key_col = 'github_url'\n",
    "scraped_key_col = 'url'\n",
    "\n",
    "if api_key_col not in api_df.columns:\n",
    "    print(f\"Error: Key column '{api_key_col}' not found in the API data. Available columns: {api_df.columns.tolist()}\")\n",
    "    exit()\n",
    "if scraped_key_col not in scraped_df.columns:\n",
    "    print(f\"Error: Key column '{scraped_key_col}' not found in the scraped data. Available columns: {scraped_df.columns.tolist()}\")\n",
    "    exit()\n",
    "\n",
    "# Rename for consistency\n",
    "print(f\"\\nRenaming '{scraped_key_col}' to '{api_key_col}' in scraped data for merging.\")\n",
    "scraped_df.rename(columns={scraped_key_col: api_key_col}, inplace=True)\n",
    "\n",
    "# --- Step 4: Merge ---\n",
    "print(f\"Merging both datasets on '{api_key_col}'...\")\n",
    "merged_df = pd.merge(api_df, scraped_df, on=api_key_col, how='left')\n",
    "print(f\"Merge complete. Shape of merged DataFrame: {merged_df.shape}\")\n",
    "\n",
    "# --- Step 5: Save the result ---\n",
    "print(f\"\\nSaving merged data to '{output_file}'...\")\n",
    "try:\n",
    "    merged_df.to_json(output_file, orient='records', lines=True, force_ascii=False)\n",
    "    print(f\"Successfully saved to '{output_file}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving the output file: {e}\")\n",
    "\n",
    "print(\"\\nScript finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f24b009d-afa3-4fc1-99c5-fe2618635ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for subfolders in 'github_data_compressed'...\n",
      "Found 0 subfolders. Processing...\n",
      "\n",
      "Error: No dataframes were loaded. Exiting.\n",
      "\n",
      "Concatenating data from all subfolders...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m     exit()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConcatenating data from all subfolders...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m api_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_dataframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully combined data. Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_df\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# --- Step 2: Read the scraped data JSONL file ---\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[1;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[1;34m(self, objs, keys)\u001b[0m\n\u001b[0;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Configuration ---\n",
    "api_data_folder = Path(\"github_data_compressed\")\n",
    "scraped_data_file = Path(\"scraped_data.jsonl\")\n",
    "output_file = Path(\"combined_github_data.jsonl\")\n",
    "\n",
    "# --- Step 1: Read and combine all JSONL files from each subfolder ---\n",
    "print(f\"Searching for subfolders in '{api_data_folder}'...\")\n",
    "all_api_folders = [f for f in api_data_folder.iterdir() if f.is_dir()]\n",
    "api_dataframes = []\n",
    "\n",
    "print(f\"Found {len(all_api_folders)} subfolders. Processing...\")\n",
    "\n",
    "for folder_path in all_api_folders:\n",
    "    print(f\"  - Processing folder '{folder_path.name}'...\")\n",
    "    \n",
    "    jsonl_file_path = folder_path / f\"{folder_path.name}.jsonl\"\n",
    "\n",
    "    if jsonl_file_path.exists():\n",
    "        try:\n",
    "            df = pd.read_json(jsonl_file_path, lines=True)\n",
    "            if not df.empty:\n",
    "                api_dataframes.append(df)\n",
    "                print(f\"    - Successfully read {len(df)} records from '{jsonl_file_path.name}'\")\n",
    "            else:\n",
    "                print(f\"    - Warning: File '{jsonl_file_path.name}' is empty.\")\n",
    "        except Exception as e:\n",
    "            print(f\"    - Error reading '{jsonl_file_path.name}': {e}\")\n",
    "    else:\n",
    "        print(f\"    - Warning: File '{jsonl_file_path.name}' not found in folder '{folder_path.name}'. Skipping.\")\n",
    "\n",
    "if not api_dataframes:\n",
    "    print(f\"\\nError: No dataframes were loaded. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "print(\"\\nConcatenating data from all subfolders...\")\n",
    "api_df = pd.concat(api_dataframes, ignore_index=True)\n",
    "print(f\"Successfully combined data. Shape: {api_df.shape}\")\n",
    "\n",
    "# --- Step 2: Read the scraped data JSONL file ---\n",
    "print(f\"\\nReading scraped data file '{scraped_data_file}'...\")\n",
    "try:\n",
    "    scraped_df = pd.read_json(scraped_data_file, lines=True)\n",
    "    print(f\"Successfully read scraped data. Shape: {scraped_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Scraped data file '{scraped_data_file}' not found. Exiting.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error reading scraped data file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Step 3: Prepare for Merging ---\n",
    "api_key_col = 'github_url'\n",
    "scraped_key_col = 'url'\n",
    "\n",
    "if api_key_col not in api_df.columns:\n",
    "    print(f\"Error: Key column '{api_key_col}' not found in the API data. Available columns: {api_df.columns.tolist()}\")\n",
    "    exit()\n",
    "if scraped_key_col not in scraped_df.columns:\n",
    "    print(f\"Error: Key column '{scraped_key_col}' not found in the scraped data. Available columns: {scraped_df.columns.tolist()}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\nRenaming '{scraped_key_col}' to '{api_key_col}' for merging...\")\n",
    "scraped_df.rename(columns={scraped_key_col: api_key_col}, inplace=True)\n",
    "\n",
    "# --- Step 4: Merge ---\n",
    "print(f\"Merging datasets using '{api_key_col}'...\")\n",
    "merged_df = pd.merge(api_df, scraped_df, on=api_key_col, how='left')\n",
    "print(f\"Merge complete. Final shape: {merged_df.shape}\")\n",
    "\n",
    "# --- Step 5: Save result ---\n",
    "print(f\"\\nSaving merged data to '{output_file}'...\")\n",
    "try:\n",
    "    merged_df.to_json(output_file, orient='records', lines=True, force_ascii=False)\n",
    "    print(f\"Data saved to '{output_file}' successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving file: {e}\")\n",
    "\n",
    "print(\"\\nScript finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32d86a7e-6f5f-4a12-91e4-a4b7d645f237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "472a1fed-fea5-4cf6-8435-4f5484625a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for archive files named '*.jsonl' in 'github_data_compressed'...\n",
      "  - No '*.jsonl' files found. Searching for '*.zip' instead...\n",
      "Found 0 zip archive file(s). Processing them...\n",
      "\n",
      "Error: Could not read any valid data from files in 'github_data_compressed'. Exiting.\n",
      "\n",
      "Concatenating data from all sources...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 82\u001b[0m\n\u001b[0;32m     79\u001b[0m     exit()\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConcatenating data from all sources...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 82\u001b[0m api_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_dataframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully combined data. Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_df\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# --- Step 2: Read the Scraped Data JSONL File ---\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[1;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[1;34m(self, objs, keys)\u001b[0m\n\u001b[0;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the path to the folder containing the archive files.\n",
    "# Some files might be plain JSONL and others might be zipped.\n",
    "api_data_folder = Path(\"github_data_compressed\")\n",
    "\n",
    "# Define the path to the single scraped data JSONL file.\n",
    "scraped_data_file = Path(\"scraped_data.jsonl\")\n",
    "\n",
    "# Define the name for the final combined output file.\n",
    "output_file = Path(\"combined_github_data.jsonl\")\n",
    "\n",
    "# --- Step 1: Read and Combine All API Data Files ---\n",
    "# First, look for plain JSONL files.\n",
    "print(f\"Searching for archive files named '*.jsonl' in '{api_data_folder}'...\")\n",
    "all_api_archive_files = list(api_data_folder.glob('*.jsonl'))\n",
    "\n",
    "api_dataframes = []  # List to hold DataFrames from each source\n",
    "\n",
    "if all_api_archive_files:\n",
    "    print(f\"Found {len(all_api_archive_files)} JSONL file(s). Processing them directly...\")\n",
    "    for jsonl_file_path in all_api_archive_files:\n",
    "        print(f\"  - Reading {jsonl_file_path.name}...\")\n",
    "        try:\n",
    "            df = pd.read_json(jsonl_file_path, lines=True)\n",
    "            if not df.empty:\n",
    "                api_dataframes.append(df)\n",
    "                print(f\"    - Successfully read {len(df)} records from {jsonl_file_path.name}.\")\n",
    "            else:\n",
    "                print(f\"    - Warning: {jsonl_file_path.name} is empty or could not be parsed.\")\n",
    "        except Exception as e:\n",
    "            print(f\"    - Error reading file {jsonl_file_path.name}: {e}\")\n",
    "else:\n",
    "    # If no plain JSONL files are found, try looking for zip archives.\n",
    "    print(\"  - No '*.jsonl' files found. Searching for '*.zip' instead...\")\n",
    "    zip_files = list(api_data_folder.glob('*.zip'))\n",
    "    print(f\"Found {len(zip_files)} zip archive file(s). Processing them...\")\n",
    "    for archive_file_path in zip_files:\n",
    "        print(f\"  - Processing '{archive_file_path.name}'...\")\n",
    "        try:\n",
    "            with zipfile.ZipFile(archive_file_path, 'r') as zip_ref:\n",
    "                # Find JSONL files within the zip archive.\n",
    "                jsonl_files_in_zip = [f for f in zip_ref.namelist() if f.endswith('.jsonl')]\n",
    "                if not jsonl_files_in_zip:\n",
    "                    print(f\"    - Warning: No JSONL file found inside '{archive_file_path.name}'. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # If multiple JSONL files are found, try to match using the archive name.\n",
    "                if len(jsonl_files_in_zip) > 1:\n",
    "                    base_name = archive_file_path.stem  # Get filename without extension.\n",
    "                    matching_files = [f for f in jsonl_files_in_zip if Path(f).stem == base_name]\n",
    "                    if matching_files:\n",
    "                        jsonl_filename_inside_zip = matching_files[0]\n",
    "                        print(f\"    - Warning: Multiple JSONL files found. Using matching name: '{jsonl_filename_inside_zip}'.\")\n",
    "                    else:\n",
    "                        jsonl_filename_inside_zip = jsonl_files_in_zip[0]\n",
    "                        print(f\"    - Warning: Multiple JSONL files found. No clear match. Using the first one: '{jsonl_filename_inside_zip}'.\")\n",
    "                else:\n",
    "                    jsonl_filename_inside_zip = jsonl_files_in_zip[0]\n",
    "\n",
    "                # Open and read the JSONL file from within the zip archive.\n",
    "                with zip_ref.open(jsonl_filename_inside_zip) as jsonl_file:\n",
    "                    df = pd.read_json(jsonl_file, lines=True)\n",
    "                    if not df.empty:\n",
    "                        api_dataframes.append(df)\n",
    "                        print(f\"    - Successfully read '{jsonl_filename_inside_zip}' ({len(df)} records).\")\n",
    "                    else:\n",
    "                        print(f\"    - Warning: '{jsonl_filename_inside_zip}' inside '{archive_file_path.name}' is empty or could not be parsed.\")\n",
    "        except zipfile.BadZipFile:\n",
    "            print(f\"  - Error: '{archive_file_path.name}' is not a valid zip file or is corrupted. Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - Error processing archive file {archive_file_path}: {e}\")\n",
    "\n",
    "if not api_dataframes:\n",
    "    print(f\"\\nError: Could not read any valid data from files in '{api_data_folder}'. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "print(\"\\nConcatenating data from all sources...\")\n",
    "api_df = pd.concat(api_dataframes, ignore_index=True)\n",
    "print(f\"Successfully combined data. Shape: {api_df.shape}\")\n",
    "\n",
    "# --- Step 2: Read the Scraped Data JSONL File ---\n",
    "print(f\"\\nReading scraped data file '{scraped_data_file}'...\")\n",
    "try:\n",
    "    scraped_df = pd.read_json(scraped_data_file, lines=True)\n",
    "    print(f\"Successfully read scraped data. Shape: {scraped_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Scraped data file '{scraped_data_file}' not found. Exiting.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error reading scraped data file {scraped_data_file}: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Step 3: Prepare for Merging ---\n",
    "api_key_col = 'github_url'\n",
    "scraped_key_col = 'url'\n",
    "\n",
    "# Confirm that both key columns exist.\n",
    "if api_key_col not in api_df.columns:\n",
    "    print(f\"Error: Key column '{api_key_col}' not found in API data. Available columns: {api_df.columns.tolist()}\")\n",
    "    exit()\n",
    "if scraped_key_col not in scraped_df.columns:\n",
    "    print(f\"Error: Key column '{scraped_key_col}' not found in scraped data. Available columns: {scraped_df.columns.tolist()}\")\n",
    "    exit()\n",
    "\n",
    "# Rename the key column in the scraped data to match the API data.\n",
    "print(f\"\\nRenaming '{scraped_key_col}' to '{api_key_col}' in scraped data for merging.\")\n",
    "scraped_df.rename(columns={scraped_key_col: api_key_col}, inplace=True)\n",
    "\n",
    "# --- Step 4: Merge the DataFrames ---\n",
    "print(f\"Merging the datasets using '{api_key_col}' as the key...\")\n",
    "merged_df = pd.merge(api_df, scraped_df, on=api_key_col, how='left')\n",
    "print(f\"Merge complete. Shape of final DataFrame: {merged_df.shape}\")\n",
    "\n",
    "# --- Step 5: Save the Combined Data ---\n",
    "print(f\"\\nSaving the merged data to '{output_file}'...\")\n",
    "try:\n",
    "    merged_df.to_json(output_file, orient='records', lines=True, force_ascii=False)\n",
    "    print(f\"Successfully saved combined data to '{output_file}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving the output file: {e}\")\n",
    "\n",
    "print(\"\\nScript finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce03a7d8-c47d-4230-926c-ae48270cf42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\PROBOOK\\Classic Isaac\\DataGospel\\GitHub\n",
      "Files in 'github_data_compressed': ['repos_2021-01-01_to_2021-03-31.jsonl.gz', 'repos_2021-04-01_to_2021-06-30.jsonl.gz', 'repos_2021-07-01_to_2021-09-30.jsonl.gz', 'repos_2021-10-01_to_2021-12-31.jsonl.gz', 'repos_2022-01-01_to_2022-03-31.jsonl.gz', 'repos_2022-04-01_to_2022-06-30.jsonl.gz', 'repos_2022-07-01_to_2022-09-30.jsonl.gz', 'repos_2022-10-01_to_2022-12-31.jsonl.gz', 'repos_2023-01-01_to_2023-03-31.jsonl.gz', 'repos_2023-04-01_to_2023-06-30.jsonl.gz', 'repos_2023-07-01_to_2023-09-30.jsonl.gz', 'repos_2023-10-01_to_2023-12-31.jsonl.gz', 'repos_2024-01-01_to_2024-03-31.jsonl.gz', 'repos_2024-04-01_to_2024-06-30.jsonl.gz', 'repos_2024-07-01_to_2024-09-30.jsonl.gz']\n",
      "\n",
      "Searching for '*.jsonl' files in 'github_data_compressed'...\n",
      "No plain JSONL files found. Searching for '*.zip' files instead...\n",
      "Found 0 zip file(s).\n",
      "\n",
      "Error: No valid API data was read from 'github_data_compressed'.\n",
      "Check that the folder contains JSONL or ZIP files and they are not empty.\n",
      "\n",
      "Concatenating data from all files...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 88\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# --- Combine All API DataFrames ---\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConcatenating data from all files...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 88\u001b[0m api_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_dataframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCombined API data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapi_df\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# --- Step 2: Read the Scraped Data JSONL File ---\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\reshape\\concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[1;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_keys_and_objs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[0;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\reshape\\concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[1;34m(self, objs, keys)\u001b[0m\n\u001b[0;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "\n",
    "# --- Configuration ---\n",
    "# The folder where your API data files are stored.\n",
    "api_data_folder = Path(\"github_data_compressed\")\n",
    "# The file that has your scraped data.\n",
    "scraped_data_file = Path(\"scraped_data.jsonl\")\n",
    "# The output file where merged data is saved.\n",
    "output_file = Path(\"combined_github_data.jsonl\")\n",
    "\n",
    "# --- Debug Info: Print current directory and folder contents ---\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "if not api_data_folder.exists():\n",
    "    print(f\"Error: The folder '{api_data_folder}' does not exist. Check your file path.\")\n",
    "    exit()\n",
    "else:\n",
    "    print(f\"Files in '{api_data_folder}': {[f.name for f in api_data_folder.iterdir()]}\")\n",
    "\n",
    "# --- Step 1: Read and Combine All API Data Files ---\n",
    "# First, try to find plain JSONL files.\n",
    "print(f\"\\nSearching for '*.jsonl' files in '{api_data_folder}'...\")\n",
    "all_api_archive_files = list(api_data_folder.glob('*.jsonl'))\n",
    "api_dataframes = []  # This list will hold your DataFrames.\n",
    "\n",
    "if all_api_archive_files:\n",
    "    print(f\"Found {len(all_api_archive_files)} JSONL file(s).\")\n",
    "    for jsonl_file_path in all_api_archive_files:\n",
    "        print(f\"  - Reading file: {jsonl_file_path.name}\")\n",
    "        try:\n",
    "            df = pd.read_json(jsonl_file_path, lines=True)\n",
    "            print(f\"    - {jsonl_file_path.name} has {len(df)} record(s).\")\n",
    "            if not df.empty:\n",
    "                api_dataframes.append(df)\n",
    "            else:\n",
    "                print(f\"    - Warning: {jsonl_file_path.name} is empty.\")\n",
    "        except Exception as e:\n",
    "            print(f\"    - Error reading {jsonl_file_path.name}: {e}\")\n",
    "else:\n",
    "    # If no JSONL files are found, then try zip files.\n",
    "    print(\"No plain JSONL files found. Searching for '*.zip' files instead...\")\n",
    "    zip_files = list(api_data_folder.glob('*.zip'))\n",
    "    print(f\"Found {len(zip_files)} zip file(s).\")\n",
    "    for archive_file_path in zip_files:\n",
    "        print(f\"  - Processing ZIP file: {archive_file_path.name}\")\n",
    "        try:\n",
    "            with zipfile.ZipFile(archive_file_path, 'r') as zip_ref:\n",
    "                # Look for JSONL files inside the ZIP.\n",
    "                jsonl_files_in_zip = [f for f in zip_ref.namelist() if f.endswith('.jsonl')]\n",
    "                if not jsonl_files_in_zip:\n",
    "                    print(f\"    - Warning: No JSONL file found in {archive_file_path.name}.\")\n",
    "                    continue\n",
    "                # If there are multiple JSONL files, try to pick one that matches the archive name.\n",
    "                if len(jsonl_files_in_zip) > 1:\n",
    "                    base_name = archive_file_path.stem\n",
    "                    matching_files = [f for f in jsonl_files_in_zip if Path(f).stem == base_name]\n",
    "                    if matching_files:\n",
    "                        jsonl_filename_inside_zip = matching_files[0]\n",
    "                        print(f\"    - Multiple JSONL files found. Using matching file: {jsonl_filename_inside_zip}\")\n",
    "                    else:\n",
    "                        jsonl_filename_inside_zip = jsonl_files_in_zip[0]\n",
    "                        print(f\"    - Multiple JSONL files found. Using first file: {jsonl_filename_inside_zip}\")\n",
    "                else:\n",
    "                    jsonl_filename_inside_zip = jsonl_files_in_zip[0]\n",
    "                    print(f\"    - Found JSONL file: {jsonl_filename_inside_zip}\")\n",
    "                with zip_ref.open(jsonl_filename_inside_zip) as jsonl_file:\n",
    "                    df = pd.read_json(jsonl_file, lines=True)\n",
    "                    print(f\"    - {jsonl_filename_inside_zip} has {len(df)} record(s).\")\n",
    "                    if not df.empty:\n",
    "                        api_dataframes.append(df)\n",
    "                    else:\n",
    "                        print(f\"    - Warning: {jsonl_filename_inside_zip} is empty or could not be parsed.\")\n",
    "        except zipfile.BadZipFile:\n",
    "            print(f\"  - Error: {archive_file_path.name} is not a valid ZIP file or is corrupted.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - Error processing {archive_file_path.name}: {e}\")\n",
    "\n",
    "# If no valid data was read, show folder contents to help debug.\n",
    "if not api_dataframes:\n",
    "    print(f\"\\nError: No valid API data was read from '{api_data_folder}'.\")\n",
    "    print(\"Check that the folder contains JSONL or ZIP files and they are not empty.\")\n",
    "    exit()\n",
    "\n",
    "# --- Combine All API DataFrames ---\n",
    "print(\"\\nConcatenating data from all files...\")\n",
    "api_df = pd.concat(api_dataframes, ignore_index=True)\n",
    "print(f\"Combined API data shape: {api_df.shape}\")\n",
    "\n",
    "# --- Step 2: Read the Scraped Data JSONL File ---\n",
    "print(f\"\\nReading scraped data from '{scraped_data_file}'...\")\n",
    "try:\n",
    "    scraped_df = pd.read_json(scraped_data_file, lines=True)\n",
    "    print(f\"Scraped data has {len(scraped_df)} record(s). Shape: {scraped_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Scraped data file '{scraped_data_file}' not found.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error reading scraped data file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Step 3: Prepare for Merging ---\n",
    "api_key_col = 'github_url'\n",
    "scraped_key_col = 'url'\n",
    "\n",
    "if api_key_col not in api_df.columns:\n",
    "    print(f\"Error: '{api_key_col}' not found in API data. Available columns: {api_df.columns.tolist()}\")\n",
    "    exit()\n",
    "if scraped_key_col not in scraped_df.columns:\n",
    "    print(f\"Error: '{scraped_key_col}' not found in scraped data. Available columns: {scraped_df.columns.tolist()}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\nRenaming '{scraped_key_col}' to '{api_key_col}' in scraped data for merge.\")\n",
    "scraped_df.rename(columns={scraped_key_col: api_key_col}, inplace=True)\n",
    "\n",
    "# --- Step 4: Merge DataFrames ---\n",
    "print(f\"Merging data on key column '{api_key_col}'...\")\n",
    "merged_df = pd.merge(api_df, scraped_df, on=api_key_col, how='left')\n",
    "print(f\"Merged data shape: {merged_df.shape}\")\n",
    "\n",
    "# --- Step 5: Save the Merged Data ---\n",
    "print(f\"\\nSaving merged data to '{output_file}'...\")\n",
    "try:\n",
    "    merged_df.to_json(output_file, orient='records', lines=True, force_ascii=False)\n",
    "    print(f\"Merged data saved successfully to '{output_file}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving merged data: {e}\")\n",
    "\n",
    "print(\"\\nScript finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "218ebaa2-608e-42f6-add8-2b229a26df59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\PROBOOK\\Classic Isaac\\DataGospel\\GitHub\n",
      "Files in 'github_data_compressed': ['repos_2021-01-01_to_2021-03-31.jsonl.gz', 'repos_2021-04-01_to_2021-06-30.jsonl.gz', 'repos_2021-07-01_to_2021-09-30.jsonl.gz', 'repos_2021-10-01_to_2021-12-31.jsonl.gz', 'repos_2022-01-01_to_2022-03-31.jsonl.gz', 'repos_2022-04-01_to_2022-06-30.jsonl.gz', 'repos_2022-07-01_to_2022-09-30.jsonl.gz', 'repos_2022-10-01_to_2022-12-31.jsonl.gz', 'repos_2023-01-01_to_2023-03-31.jsonl.gz', 'repos_2023-04-01_to_2023-06-30.jsonl.gz', 'repos_2023-07-01_to_2023-09-30.jsonl.gz', 'repos_2023-10-01_to_2023-12-31.jsonl.gz', 'repos_2024-01-01_to_2024-03-31.jsonl.gz', 'repos_2024-04-01_to_2024-06-30.jsonl.gz', 'repos_2024-07-01_to_2024-09-30.jsonl.gz']\n",
      "\n",
      "Searching for '*.jsonl' and '*.jsonl.gz' files in 'github_data_compressed'...\n",
      "Found 15 file(s). Processing each file...\n",
      "  - Reading file: repos_2021-01-01_to_2021-03-31.jsonl.gz\n",
      "    - repos_2021-01-01_to_2021-03-31.jsonl.gz has 1000 record(s).\n",
      "  - Reading file: repos_2021-04-01_to_2021-06-30.jsonl.gz\n",
      "    - repos_2021-04-01_to_2021-06-30.jsonl.gz has 1000 record(s).\n",
      "  - Reading file: repos_2021-07-01_to_2021-09-30.jsonl.gz\n",
      "    - repos_2021-07-01_to_2021-09-30.jsonl.gz has 1000 record(s).\n",
      "  - Reading file: repos_2021-10-01_to_2021-12-31.jsonl.gz\n",
      "    - repos_2021-10-01_to_2021-12-31.jsonl.gz has 1000 record(s).\n",
      "  - Reading file: repos_2022-01-01_to_2022-03-31.jsonl.gz\n",
      "    - repos_2022-01-01_to_2022-03-31.jsonl.gz has 1000 record(s).\n",
      "  - Reading file: repos_2022-04-01_to_2022-06-30.jsonl.gz\n",
      "    - repos_2022-04-01_to_2022-06-30.jsonl.gz has 1000 record(s).\n",
      "  - Reading file: repos_2022-07-01_to_2022-09-30.jsonl.gz\n",
      "    - repos_2022-07-01_to_2022-09-30.jsonl.gz has 1000 record(s).\n",
      "  - Reading file: repos_2022-10-01_to_2022-12-31.jsonl.gz\n",
      "    - repos_2022-10-01_to_2022-12-31.jsonl.gz has 1000 record(s).\n",
      "  - Reading file: repos_2023-01-01_to_2023-03-31.jsonl.gz\n",
      "    - repos_2023-01-01_to_2023-03-31.jsonl.gz has 1000 record(s).\n",
      "  - Reading file: repos_2023-04-01_to_2023-06-30.jsonl.gz\n",
      "    - repos_2023-04-01_to_2023-06-30.jsonl.gz has 1000 record(s).\n",
      "  - Reading file: repos_2023-07-01_to_2023-09-30.jsonl.gz\n",
      "    - repos_2023-07-01_to_2023-09-30.jsonl.gz has 644 record(s).\n",
      "  - Reading file: repos_2023-10-01_to_2023-12-31.jsonl.gz\n",
      "    - repos_2023-10-01_to_2023-12-31.jsonl.gz has 1000 record(s).\n",
      "  - Reading file: repos_2024-01-01_to_2024-03-31.jsonl.gz\n",
      "    - repos_2024-01-01_to_2024-03-31.jsonl.gz has 1000 record(s).\n",
      "  - Reading file: repos_2024-04-01_to_2024-06-30.jsonl.gz\n",
      "    - repos_2024-04-01_to_2024-06-30.jsonl.gz has 1000 record(s).\n",
      "  - Reading file: repos_2024-07-01_to_2024-09-30.jsonl.gz\n",
      "    - repos_2024-07-01_to_2024-09-30.jsonl.gz has 1000 record(s).\n",
      "\n",
      "Concatenating data from all files...\n",
      "Combined API data shape: (14644, 50)\n",
      "\n",
      "Reading scraped data from 'scraped_data.jsonl'...\n",
      "Scraped data has 14641 record(s). Shape: (14641, 6)\n",
      "\n",
      "Renaming 'url' to 'github_url' in scraped data for merging.\n",
      "Merging data on key column 'github_url'...\n",
      "Merged data shape: (14644, 55)\n",
      "\n",
      "Saving merged data to 'combined_github_data.jsonl'...\n",
      "Merged data saved successfully to 'combined_github_data.jsonl'.\n",
      "\n",
      "Script finished.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "# Folder containing the API data files.\n",
    "api_data_folder = Path(\"github_data_compressed\")\n",
    "# File containing the scraped data.\n",
    "scraped_data_file = Path(\"scraped_data.jsonl\")  # Change to scraped_data.jsonl.gz if needed\n",
    "# Output file for the merged data.\n",
    "output_file = Path(\"combined_github_data.jsonl\")\n",
    "\n",
    "# --- Debug Info: Print current directory and list folder contents ---\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "if not api_data_folder.exists():\n",
    "    print(f\"Error: The folder '{api_data_folder}' does not exist. Check your file path.\")\n",
    "    exit()\n",
    "else:\n",
    "    print(f\"Files in '{api_data_folder}': {[f.name for f in api_data_folder.iterdir()]}\")\n",
    "\n",
    "# --- Step 1: Read and Combine All API Data Files ---\n",
    "# Look for both plain JSONL and gzipped JSONL files.\n",
    "print(f\"\\nSearching for '*.jsonl' and '*.jsonl.gz' files in '{api_data_folder}'...\")\n",
    "jsonl_files = list(api_data_folder.glob('*.jsonl'))\n",
    "jsonl_gz_files = list(api_data_folder.glob('*.jsonl.gz'))\n",
    "all_api_archive_files = jsonl_files + jsonl_gz_files\n",
    "\n",
    "api_dataframes = []  # List to store DataFrames\n",
    "\n",
    "if all_api_archive_files:\n",
    "    print(f\"Found {len(all_api_archive_files)} file(s). Processing each file...\")\n",
    "    for file_path in all_api_archive_files:\n",
    "        print(f\"  - Reading file: {file_path.name}\")\n",
    "        try:\n",
    "            # Compression will be inferred from the file extension.\n",
    "            df = pd.read_json(file_path, lines=True, compression='infer')\n",
    "            print(f\"    - {file_path.name} has {len(df)} record(s).\")\n",
    "            if not df.empty:\n",
    "                api_dataframes.append(df)\n",
    "            else:\n",
    "                print(f\"    - Warning: {file_path.name} is empty.\")\n",
    "        except Exception as e:\n",
    "            print(f\"    - Error reading {file_path.name}: {e}\")\n",
    "else:\n",
    "    # If no JSONL or gzipped JSONL files are found, attempt to search for ZIP files.\n",
    "    print(\"No '*.jsonl' or '*.jsonl.gz' files found. Searching for '*.zip' files instead...\")\n",
    "    zip_files = list(api_data_folder.glob('*.zip'))\n",
    "    print(f\"Found {len(zip_files)} zip file(s).\")\n",
    "    for archive_file_path in zip_files:\n",
    "        print(f\"  - Processing ZIP file: {archive_file_path.name}\")\n",
    "        try:\n",
    "            with zipfile.ZipFile(archive_file_path, 'r') as zip_ref:\n",
    "                jsonl_files_in_zip = [f for f in zip_ref.namelist() if f.endswith('.jsonl')]\n",
    "                if not jsonl_files_in_zip:\n",
    "                    print(f\"    - Warning: No JSONL file found in {archive_file_path.name}.\")\n",
    "                    continue\n",
    "                if len(jsonl_files_in_zip) > 1:\n",
    "                    base_name = archive_file_path.stem\n",
    "                    matching_files = [f for f in jsonl_files_in_zip if Path(f).stem == base_name]\n",
    "                    if matching_files:\n",
    "                        jsonl_filename_inside_zip = matching_files[0]\n",
    "                        print(f\"    - Multiple JSONL files found. Using matching file: {jsonl_filename_inside_zip}\")\n",
    "                    else:\n",
    "                        jsonl_filename_inside_zip = jsonl_files_in_zip[0]\n",
    "                        print(f\"    - Multiple JSONL files found. Using first file: {jsonl_filename_inside_zip}\")\n",
    "                else:\n",
    "                    jsonl_filename_inside_zip = jsonl_files_in_zip[0]\n",
    "                    print(f\"    - Found JSONL file: {jsonl_filename_inside_zip}\")\n",
    "                with zip_ref.open(jsonl_filename_inside_zip) as jsonl_file:\n",
    "                    df = pd.read_json(jsonl_file, lines=True)\n",
    "                    print(f\"    - {jsonl_filename_inside_zip} has {len(df)} record(s).\")\n",
    "                    if not df.empty:\n",
    "                        api_dataframes.append(df)\n",
    "                    else:\n",
    "                        print(f\"    - Warning: {jsonl_filename_inside_zip} is empty or could not be parsed.\")\n",
    "        except zipfile.BadZipFile:\n",
    "            print(f\"  - Error: {archive_file_path.name} is not a valid ZIP file or is corrupted.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - Error processing {archive_file_path.name}: {e}\")\n",
    "\n",
    "if not api_dataframes:\n",
    "    print(f\"\\nError: No valid API data was read from '{api_data_folder}'.\")\n",
    "    print(\"Check that the folder contains .jsonl, .jsonl.gz or ZIP files and that they are not empty.\")\n",
    "    exit()\n",
    "\n",
    "# --- Combine All API DataFrames ---\n",
    "print(\"\\nConcatenating data from all files...\")\n",
    "api_df = pd.concat(api_dataframes, ignore_index=True)\n",
    "print(f\"Combined API data shape: {api_df.shape}\")\n",
    "\n",
    "# --- Step 2: Read the Scraped Data JSONL File ---\n",
    "print(f\"\\nReading scraped data from '{scraped_data_file}'...\")\n",
    "try:\n",
    "    # Use compression='infer' in case the scraped file is gzipped.\n",
    "    scraped_df = pd.read_json(scraped_data_file, lines=True, compression='infer')\n",
    "    print(f\"Scraped data has {len(scraped_df)} record(s). Shape: {scraped_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Scraped data file '{scraped_data_file}' not found. Exiting.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error reading scraped data file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Step 3: Prepare for Merging ---\n",
    "api_key_col = 'github_url'\n",
    "scraped_key_col = 'url'\n",
    "\n",
    "if api_key_col not in api_df.columns:\n",
    "    print(f\"Error: Key column '{api_key_col}' not found in API data. Available columns: {api_df.columns.tolist()}\")\n",
    "    exit()\n",
    "if scraped_key_col not in scraped_df.columns:\n",
    "    print(f\"Error: Key column '{scraped_key_col}' not found in scraped data. Available columns: {scraped_df.columns.tolist()}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\nRenaming '{scraped_key_col}' to '{api_key_col}' in scraped data for merging.\")\n",
    "scraped_df.rename(columns={scraped_key_col: api_key_col}, inplace=True)\n",
    "\n",
    "# --- Step 4: Merge DataFrames ---\n",
    "print(f\"Merging data on key column '{api_key_col}'...\")\n",
    "merged_df = pd.merge(api_df, scraped_df, on=api_key_col, how='left')\n",
    "print(f\"Merged data shape: {merged_df.shape}\")\n",
    "\n",
    "# --- Step 5: Save the Merged Data ---\n",
    "print(f\"\\nSaving merged data to '{output_file}'...\")\n",
    "try:\n",
    "    merged_df.to_json(output_file, orient='records', lines=True, force_ascii=False)\n",
    "    print(f\"Merged data saved successfully to '{output_file}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving merged data: {e}\")\n",
    "\n",
    "print(\"\\nScript finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ffef49-353e-46ae-b8ed-b88f51ff65a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
